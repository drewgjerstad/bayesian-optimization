{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4da9584",
   "metadata": {},
   "source": [
    "# Bayesian Decision Theory\n",
    "**Drew Gjerstad**  \n",
    "\n",
    "_Bayesian Optimization Series_  \n",
    "[github.com/drewgjerstad/bayesian-optimization](https://github.com/drewgjerstad/bayesian-optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16471b",
   "metadata": {},
   "source": [
    "The optimization process in its most basic form is a series of decisions.\n",
    "Ideally, these decisions are made via a principled approach (i.e.,\n",
    "strategically) which is where **decision theory** comes into play. Specifically,\n",
    "the approach used to make such decisions should take into account any available\n",
    "data when deciding where each observation is made. Unfortunately, it is not\n",
    "clear how to make these decisions primarily due to the likely incomplete and\n",
    "ever-changing information about the objective function. \n",
    "\n",
    "Previously, we discussed how to use Bayesian inference as a framework that\n",
    "systematically and quantitatively reasons about the uncertainty in the objective\n",
    "function. This is one of the main difficulties when making decisions during\n",
    "optimization since our knowledge of the objective function is only updated from\n",
    "the outcomes of our own decisions.\n",
    "\n",
    "In this notebook, we focus on Bayesian decision theory which in effect, \"bridges\n",
    "the gap\" between Bayesian inference and decision making in optimization. This\n",
    "principled approach allows us to make decisions using a probabilistic belief\n",
    "about the objective function to guide optimization policies under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e760e63",
   "metadata": {},
   "source": [
    "At the heart of the optimization process is the _optimization policy_ which\n",
    "determines where we will make an observation next (for the time being, ignoring\n",
    "the question of termination), acquires the next observation, and updates our\n",
    "knowledge of the objective. Therefore, we aim to obtain the **optimal policy**\n",
    "with optimal referring to maximizing the expected utility and quality of the\n",
    "observed data.\n",
    "\n",
    "While the idea of deriving this optimal policy may seem simple, especially when\n",
    "deriving it in a theoretical manner, the theoretically optimal policy is often\n",
    "impossible to compute and has little practical value. Regardless, the process of\n",
    "deriving this policy will enable us to see how we can obtain effective\n",
    "_approximations_.\n",
    "\n",
    "Coming back to the question of termination, this question itself represents a\n",
    "decision that is crucial in several applications. A **stopping rule** is a\n",
    "procedure that decides whether to terminate or continue optimization based on\n",
    "the observed data. In many cases, this rule is _deterministic_ meaning it is\n",
    "fixed and known before we begin optimizing. One example is a preallocated search\n",
    "budget defined by the maximum number of allowed observations. This type of\n",
    "stopping rule will terminate the optimization once we obtain the maximum number\n",
    "of allowed observations, regardless of our progress.\n",
    "\n",
    "Alternatively, we may want to consider the optimization progress (i.e., our\n",
    "understanding of the objective function and the expected cost of continuing)\n",
    "when deciding whether to terminate or not. This is a more _dynamic_ stopping\n",
    "rule that will require more subtle, adaptive stopping rules. We will discuss\n",
    "this more later and how its formulation inspires better approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf848cf4",
   "metadata": {},
   "source": [
    "## Defining Optimization Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c5bfd",
   "metadata": {},
   "source": [
    "To define an optimization policy, we typically use an intermediate function\n",
    "called the **acquisition function** that will score each observation candidate\n",
    "based on its utility to aiding the optimization process. Then, the policy can\n",
    "be defined to observe the point deemed to be most useful (or most \"promising\")\n",
    "by the acquisition function. Such a definition is used by nearly all Bayesian\n",
    "optimization policies, with some literature (as noted by Garnett) using the term\n",
    "\"acquisition function\" interchangeably with \"policy\".\n",
    "\n",
    "When using the Bayesian approach, the acquisition function is almost always\n",
    "defined by obtaining the posterior belief (distribution) of the objective given\n",
    "the data and then defining our preferences for the next observation with respect\n",
    "to this belief. Using the notation from Garnett's book, we will denote\n",
    "$\\alpha(x;\\mathcal{D})$ for a general acquisition function with the data,\n",
    "$\\mathcal{D}$, serving as parameters that shape our preferences.\n",
    "\n",
    "In more mathematical terms, an acquisition function $\\alpha$ defines preferences\n",
    "over candidate observations by \"inducing a total order over the domain\". This\n",
    "means that given existing data $\\mathcal{D}$, observing candidate $x$ is\n",
    "preferred over another candidate $x^\\prime$ if\n",
    "$\\alpha(x;\\mathcal{D}) > \\alpha(x^\\prime;\\mathcal{D})$.  Rationally, the action\n",
    "we will prefer is one that maximizes the acquisition function:\n",
    "\n",
    "\\begin{equation*}\n",
    "    x \\in \\argmax_{x^\\prime\\in\\mathcal{X}} \\alpha(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "We can use the formulation above as a kind of \"sub-optimization problem\". Once\n",
    "it is solved, the acquisition function will map a set of observed data to a\n",
    "candidate $x \\in \\mathcal{X}$ to observe next, filling the exact role of an\n",
    "optimization policy.\n",
    "\n",
    "If you are thinking that the idea of solving global optimization problems by\n",
    "repeatedly solving global optimization problems is unintuitive, don't worry! In\n",
    "many cases, this paradox is resolved by the fact that common acquisition\n",
    "functions have properties making their optimization much more tractable than the\n",
    "primary optimization problem we are aiming to solve.\n",
    "\n",
    "Commonly used acquisition functions are both inexpensive to evaluate and are\n",
    "analytically differentiable which means we can use pre-defined optimizers while\n",
    "computing the policy formulated above. However, recall that our objective\n",
    "function is assumed to be rather expensive to evaluate and lacks efficient (if\n",
    "any at all) gradients. Using the ideas outlined here, we are able to moderate\n",
    "a difficult problem to several simpler problems, a reasonable first step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d5df0",
   "metadata": {},
   "source": [
    "## Formalizing Bayesian Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2a70e",
   "metadata": {},
   "source": [
    "Bayesian decision theory is a framework that we can use to make decisions under\n",
    "uncertainty while still being flexible enough that it can be applied to nearly\n",
    "any problem. Here, we introduce Bayesian decision theory in the same manner as\n",
    "Garnett: focusing on the key concepts through the lense of optimization rather\n",
    "than unloading the entire theory abstractly. Garnett recommends the following\n",
    "supplementary texts for a more thorough and in-depth review of the theory:\n",
    " * _Optimal Statistical Decisions_ by M. H. DeGroot\n",
    " * _Statistical Decision Theory and Bayesian Analysis_ by J. O. Berger\n",
    "\n",
    "Being sufficiently familiar with this topic can help you understand key concepts\n",
    "in Bayesian optimization that are examined in the literature less thoroughly\n",
    "than they perhaps should be. In particular, this topic, as Garnett puts it,\n",
    "serves as the \"hidden origin\" of several typical acquisition functions.\n",
    "\n",
    "Following from Garnett's text, we start with using the Bayesian decision theory\n",
    "approach for decision making and examine the case of making a single, isolated\n",
    "decision to see how the framework is used to make optimal decisions. Then, we\n",
    "will extend this reasoning to make several, or a sequence of, decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c430b6",
   "metadata": {},
   "source": [
    "### Case 1: Single, Isolated Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e37cf2c",
   "metadata": {},
   "source": [
    "There are two defining characteristics of a decision problem under uncertainty:\n",
    "the action space and the presence of uncertain elements in the environment. We\n",
    "will review these characteristics first.\n",
    "\n",
    "The **action space** $\\mathcal{A}$ is the set of all available decisions. Keep\n",
    "in mind that the task at hand is to select an action from this space. In the\n",
    "context of sequential optimization, we are selecting a point in the domain\n",
    "$\\mathcal{X}$ to observe so we have that $\\mathcal{A}=\\mathcal{X}$.\n",
    "\n",
    "The **presence of uncertain elements** in the environment will inherently\n",
    "influence the results of our actions which complicates our decision. Using\n",
    "Garnett's notation, let $\\psi$ denote a random variable that encompasses any\n",
    "relevant uncertain elements when making and evaluating a decision. While we may\n",
    "not have all the information about the uncertainty, we can use Bayesian\n",
    "inference to reason about $\\psi$ given the observed data using the posterior\n",
    "distribution $p(\\psi\\vert\\mathcal{D})$. We can use this belief to aid our\n",
    "decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b8a21",
   "metadata": {},
   "source": [
    "Suppose that now we need to make a decision (selected from the action space,\n",
    "$\\mathcal{A}$) under the uncertainty in $\\psi$, and informed by observed data\n",
    "$\\mathcal{D}$. We need some way to guide our decision selection process: a\n",
    "_utility function_.\n",
    "\n",
    "A real-valued **utility-function** $u(a, \\psi, \\mathcal{D})$ is used to guide\n",
    "our choice by measuring the quality of choosing action $a$ if the true state of\n",
    "the environment is $\\psi$, with higher utilities being preferred since the\n",
    "higher the utility score, the more favorable the outcome. Notice that the\n",
    "arguments provided to the utility function are all that is required to judge the\n",
    "quality of a decision:\n",
    " * the proposed action $a$\n",
    " * observed data informing our current knowledge $\\mathcal{D}$\n",
    " * uncertain elements missing from our knowledge $\\psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7ed05",
   "metadata": {},
   "source": [
    "Since we have incomplete information about $\\psi$, we are unable to know the\n",
    "exact utility of selecting any given action. However, we can compute the\n",
    "_expected_ utility of selecting an action $a$ based on our posterior belief:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{E}\\left[u(a,\\psi,\\mathcal{D})\\vert a,\\mathcal{D}\\right] =\n",
    "    \\int u(a,\\psi,\\mathcal{D})p(\\psi\\vert\\mathcal{D})d\\psi\n",
    "\\end{equation*}\n",
    "\n",
    "The expected utility above maps each action to a real value that induces a total\n",
    "order and provides a simple method to make our decision. We then select an\n",
    "action that maximizes the expected utility:\n",
    "\n",
    "\\begin{equation*}\n",
    "    a \\in \\argmax_{a^\\prime\\in\\mathcal{A}}\\mathbb{E}\\left[\n",
    "        u(a^\\prime,\\psi,\\mathcal{D})\\vert a^\\prime,\\mathcal{D}\\right]\n",
    "\\end{equation*}\n",
    "\n",
    "By using this approach, the decision is considered to be optimal as there are no\n",
    "other actions that would result in greater expected utility. Furthermore, this\n",
    "method of selecting actions optimally under uncertainty is the central concept\n",
    "of Bayesian decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f617f5",
   "metadata": {},
   "source": [
    "### Case 2: Sequential Decisions (with fixed budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d801aad6",
   "metadata": {},
   "source": [
    "Previously, we examined the single-decision case where we used Bayesian decision\n",
    "theory as a framework to select optimal decisions based on the data. The main\n",
    "idea was to evaluate a decision's quality after it occurs and then select\n",
    "actions that maximize the expected utility. Now, we will extend this reasoning\n",
    "to sequential decisions and specifically, the construction of optimization\n",
    "policies. This is a bit more complicated, however, as any single decision will\n",
    "impact all of the future decisions we will make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648f51e",
   "metadata": {},
   "source": [
    "To construct an optimization routine, we will need to define a policy that\n",
    "adaptively designs a sequence of observations (actions) that move us closer to\n",
    "the optimum. Using the concepts outlined previously, each choice can be modeled\n",
    "as a decision problem under uncertainty.\n",
    " * Let $\\mathcal{X}$ be the domain and action space of each decision.\n",
    " * Let $\\mathcal{f}$ be the objective function.\n",
    "\n",
    "We will utilize the idea of probabilistic beliefs during optimization so that we\n",
    "can reason about the uncertainty in the objective. Recall that this is called\n",
    "the posterior predictive distribution or posterior process,\n",
    "$p(\\mathcal{f}\\vert\\mathcal{D})$. At this time, we do not need to make any\n",
    "assumptions about this distribution nor do we need to think of it as a Gaussian\n",
    "process. However, we can use this distribution to reason about the result of an\n",
    "observation at location $x$: $p(y\\vert x,\\mathcal{D})$.\n",
    "\n",
    "Recall that the main goal of optimization is to collect and return a dataset\n",
    "$\\mathcal{D}$. This means that we need to determine what data we _want_ to\n",
    "acquire and this is accomplished by defining a utility function that evaluates\n",
    "the quality of data obtained by the optimizer. More specifically, the utility\n",
    "function establishes our preferences with the common preference being to obtain\n",
    "a dataset of higher utility than any dataset of lower utility. The utility will\n",
    "guide our policy design by \"choosing\" observations that we expect to improve\n",
    "the utility the most.\n",
    "\n",
    "In the next notebook, we will define several utility functions in detail whereas\n",
    "here we will continue developing Bayesian decision theory using an _arbitrary_\n",
    "_utility function_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747c33a",
   "metadata": {},
   "source": [
    "#### Facing Uncertainty During Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bcf556",
   "metadata": {},
   "source": [
    "During optimization, we are always facing some sort of uncertainty. However,\n",
    "the kind of impact this uncertainty has is what distinguishes the isolated,\n",
    "single-decision case from the sequential decisions case. In the single-decision\n",
    "case, we simply select the next observation $x$ that maximizes the expected\n",
    "utility. On the other hand, in the sequential decisions case, we repeatedly\n",
    "select the next observation $x$ in a similar manner (i.e., maximizing the\n",
    "expected utility) but in this case, after each observation $x$ and corresponding\n",
    "value $y$ are obtained, they are added to our dataset. This means that the\n",
    "observations in the dataset, including the observations we chose, will be used\n",
    "to make future decisions. More generally, the observations we select will impact\n",
    "the \"entire remainder of optimization\" and extra consideration compared to the\n",
    "isolated, single-decision case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5f478",
   "metadata": {},
   "source": [
    "From this realization, we can intuitively think that making decisions closer to\n",
    "termination are easier since there are less (if any) future decisions that will\n",
    "rely on their outcomes. Using this to our advantage, we will design optimization\n",
    "policies _in reverse_ where we will initially reason about the last decision.\n",
    "The last decision is made using approach of the isolated, single-decision case\n",
    "since we do not have to consider any future decisions. Then, we will continue\n",
    "reasoning backwards through decisions until the first decision, defining optimal\n",
    "behavior as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba231f",
   "metadata": {},
   "source": [
    "#### Construction of Optimization Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3757e",
   "metadata": {},
   "source": [
    "When considering the construction of optimization policies, we will assume that\n",
    "we are constrained by a pre-defined and fixed search budget representing the\n",
    "maximum number of observations we can make. In addition to being common in\n",
    "practice, this assumption also makes the analysis of policy design more\n",
    "convenient (particularly because we can ignore the question of termination).\n",
    "\n",
    "As noted by Garnett, this assumption will also imply that every observation has\n",
    "a constant acquisition cost that may not always be reasonable. Considerations\n",
    "of acquisition costs and the question of termination will be examined later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53eac68",
   "metadata": {},
   "source": [
    "Under this assumption of fixed budget, we can analyze policies using the number\n",
    "of future observations before termination--which is known. Then, using the setup\n",
    "from Garnett, the problem becomes this: given a set of data, how should we\n",
    "select our next evaluation point when exactly $\\tau$ observations remain before\n",
    "we terminate? Note that $\\tau$ denotes the **decision horizon** and indicates\n",
    "the number of remaining observations.\n",
    "\n",
    "We will define notation used by Garnett to analyze optimization policies below.\n",
    " * Let $x$ denote the location of an observation.\n",
    " * Let $y$ denote the corresponding value of an observation at location $x$.\n",
    " * Let $\\mathcal{D}_i = \\mathcal{D} \\cup \\{(x_i, y_i)\\}, i\\in\\{1,\\dots,\\tau\\}$\n",
    "   be the dataset that is available at the next stage of optimization where the\n",
    "   subscript $i$ indicates the number of future observations incorporated with\n",
    "   the current data.\n",
    "     - The dataset returned by our optimization procedure will be\n",
    "       $\\mathcal{D}_\\tau$ with utility $u(\\mathcal{D}_\\tau)$.\n",
    "\n",
    "To measure the utility of the data, we will use the same format as before:\n",
    "\n",
    "\\begin{equation*}\n",
    "    u(\\mathcal{D}_\\tau) = u(D, x, y, x_2, y_2, \\dots, x_\\tau, y_\\tau)\n",
    "\\end{equation*}\n",
    "\n",
    "This format expresses the **terminal utility** in terms of the proposed current\n",
    "action $x$, observed data $\\mathcal{D}$, and unknown future data that will be\n",
    "obtained: not-yet observed value $y$, locations $\\{x_2,\\dots,x_\\tau\\}$ and\n",
    "corresponding values $\\{y_2,\\dots,y_\\tau\\}$ of future observations.\n",
    "\n",
    "Extending the treatment of isolated, single decisions, we can evaluate an\n",
    "candidate observation at point $x$ using the expected _terminal_ utility if we\n",
    "observed that point next:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{E}\\left[u(\\mathcal{D}_\\tau)\\vert x,\\mathcal{D}\\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Just as before, we can now define an optimization policy that maximizes this\n",
    "utility:\n",
    "\n",
    "\\begin{equation*}\n",
    "    x \\in \\argmax_{x^\\prime\\in\\mathcal{X}}\n",
    "    \\mathbb{E}\\left[u(\\mathcal{D}_\\tau)\\vert x^\\prime,\\mathcal{D}\\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Conceptually, these ideas are rather simple from a theoretical perspective but\n",
    "we must consider how we will actually compute the expected terminal utility. The\n",
    "explicit form from Garnett is the expectation over future observations:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\int\\dots\\int u(\\mathcal{D}_\\tau)p(y\\vert x,\\mathcal{D})\n",
    "    \\prod_{i=2}^{\\tau}p(x_i,y_i\\vert\\mathcal{D}_{i-1})\n",
    "    \\hspace{2pt}\\text{d}y\n",
    "    \\hspace{2pt}\\text{d}\\{(x_i,y_i)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "Actually computing this integral is rather unwieldy and so we will instead\n",
    "compute this expression under the assumption that _all future decisions are_\n",
    "_made optimally_ (Bellman's Principle of Optimality). Such analysis will obtain\n",
    "the optimal optimization policy, and will be covered later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66bf964",
   "metadata": {},
   "source": [
    "For now, we will use **backward induction** to determine the optimal behavior\n",
    "if only one observation remains and continue backwards inductively to consider\n",
    "increasingly long horizons. We will use Garnett's notation for the expected\n",
    "_increase_ in utility beginning from an arbitrary dataset $\\mathcal{D}$, making\n",
    "an observation at $x$, and behaving optimally until we reach termination $\\tau$\n",
    "steps in the future. This is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(x;\\mathcal{D}) = \\mathbb{E}\n",
    "    \\left[u(\\mathcal{D}_\\tau)\\vert x,\\mathcal{D}\\right] - u(\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "Notice that this is merely the difference between the expected terminal utility\n",
    "and the utility of the existing dataset. Furthermore, such notation is similar\n",
    "to the notation used for acquisition functions and this enables us to define\n",
    "the optimal optimization policy using acquisition functions defined in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a825c2f",
   "metadata": {},
   "source": [
    "#### Base Case: One Observation Remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad4e80",
   "metadata": {},
   "source": [
    "Using the method of backward induction, we start with the case where there is\n",
    "only one observation remaining in the horizon; there are $\\tau=1$ steps left\n",
    "before termination. For this case, the terminal dataset $\\mathcal{D}_\\tau$ is\n",
    "the current dataset augmented with one additional observation. Recall that this\n",
    "case is essentially the isolated, single decision case and so we can use the\n",
    "framework developed for that case.\n",
    "\n",
    "First, we need to compute the marginal gain in utility from a final evaluation\n",
    "at $x$. This is an expectation over the corresponding value $y$ with respect to\n",
    "the posterior predictive distribution:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_1(x;\\mathcal{D}) = \\int u(\\mathcal{D}_1)p(y\\vert x,\\mathcal{D})\n",
    "    \\hspace{2pt}\\text{d}y - u(\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "Using the framework developed for the single decision case, the optimal\n",
    "observation is the observation that maximizes the expected marginal gain:\n",
    "\n",
    "\\begin{equation*}\n",
    "    x \\in \\argmax_{x^\\prime\\in\\mathcal{X}}\\alpha_1(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "This leads to the dataset returned by the optimizer having expected utility:\n",
    "\n",
    "\\begin{equation*}\n",
    "    u(\\mathcal{D}) - \\alpha_1^*(\\mathcal{D});\n",
    "    \\hspace{24pt}\n",
    "    \\alpha_1^*(x^\\prime;\\mathcal{D}) =\n",
    "    \\max_{x^\\prime\\in\\mathcal{X}} \\alpha_1(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "From this, we denote the **value** of the dataset by\n",
    "$\\alpha_\\tau^*(\\mathcal{D})$ and represents the expected increase in the\n",
    "dataset's utility if we start with arbitrary dataset $\\mathcal{D}$ and continue\n",
    "in an optimal manner for $\\tau$ more observations. This will be key in further\n",
    "analysis but for now, this concludes the _base case_.\n",
    "\n",
    "See Figure 5.1 in Garnett's text for an illustration of the optimal optimization\n",
    "policy when the decision horizon is $\\tau=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ad9e77",
   "metadata": {},
   "source": [
    "#### Special Case: Two Observations Remaining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b761281",
   "metadata": {},
   "source": [
    "Before we begin examining the inductive case, we will review a special case\n",
    "where there are two observations remaining (i.e., the decision horizon is\n",
    "$\\tau=2$). Just as in the base case, suppose we have an arbitrary dataset\n",
    "$\\mathcal{D}$ but now we need to decide where the next to last observation $x$\n",
    "should be. The reasoning developed for this special case will help to highlight\n",
    "the inductive approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2103e721",
   "metadata": {},
   "source": [
    "In the same manner as we did in the base case with $\\tau=1$, we consider the\n",
    "expected increase in terminal utility after two observations:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_2(x;\\mathcal{D}) = \\mathbb{E}\\left[\n",
    "        u(\\mathcal{D}_2)\\vert x,\\mathcal{D}\n",
    "    \\right] - u(\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "Based on the definition of expectations, the expectation above should require\n",
    "that we marginalize the observation $y$, the final observation $x_2$, and its\n",
    "corresponding value $y_2$. Luckily, we can use Bellman's Principle of Optimality\n",
    "to assume that future behavior is optimal, allowing us to simplify how we\n",
    "approach the final decision $x_2$.\n",
    "\n",
    "First, we will redefine the two-step expected gain in utility $\\alpha_2$ in\n",
    "terms of the single-step case $\\alpha_1$, a function that we have a much more\n",
    "established understanding of. From Garnett, this \"two-step difference\" in\n",
    "utility can be expressed as a _telescoping sum_:\n",
    "\n",
    "\\begin{equation*}\n",
    "    u(\\mathcal{D}_2) - u(\\mathcal{D}) =\n",
    "    \\left[u(\\mathcal{D}_1)-u(\\mathcal{D})\\right] +\n",
    "    \\left[u(\\mathcal{D}_2)-u(\\mathcal{D}_1)\\right]\n",
    "\\end{equation*}\n",
    "\n",
    "This allows us to separate the expected increase in terminal utility after two\n",
    "observations into two terms: the expected increase after the first observation\n",
    "(the **expected immediate gain**) and the expected additional increase after the\n",
    "final observation (the **expected future gain**), shown below.\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_2(x;\\mathcal{D}) = \\alpha_1(x;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_1(x_2;\\mathcal{D}_1)\\vert x,\\mathcal{D}\n",
    "    \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "While it is not fully clear how we should address the second term (the expected\n",
    "future gain), we can use our analysis of the base case to help us reason. Given\n",
    "the observation $x$'s value $y$ and knowledge of $\\mathcal{D}_1$, the _optimal_\n",
    "final observation $x_2$ will result in an expected marginal gain of\n",
    "$\\alpha_1^*(\\mathcal{D}_1)$ (a quantity that we can compute). Thus, under the\n",
    "assumption of optimal future behavior, we can express the expectation with the\n",
    "current observation $y$ only:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_2(x;\\mathcal{D}) =\n",
    "    \\alpha_1(x;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_1^*(\\mathcal{D}_1)\\vert x,\\mathcal{D}\n",
    "    \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Once again, the optimal next-to-last observation maximizes the expected gain:\n",
    "\n",
    "\\begin{equation*}\n",
    "    x \\in \\argmax_{x^\\prime\\in\\mathcal{X}}\\alpha_2(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "Furthermore, it will provide an expected terminal utility of:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\u(\\mathcal{D}) + \\alpha_2^*(\\mathcal{D})\n",
    "    \\hspace{24pt}\n",
    "    \\alpha_2^*(\\mathcal{D}) =\n",
    "    \\max_{x^\\prime\\in\\mathcal{X}}\\alpha_2(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "This analysis shows that we are able to achieve optimal behavior and compute the\n",
    "value of any dataset with a horizon of $\\tau=2$.\n",
    "\n",
    "See Figures 5.2 and 5.3 in Garnett's text for an illustration of an optimal\n",
    "two-step optimization policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e9314",
   "metadata": {},
   "source": [
    "#### Inductive Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8f1d4",
   "metadata": {},
   "source": [
    "Now that we have examined and analyzed the base and special cases with horizons\n",
    "of $\\tau=1$ and $\\tau=2$, respectively, we can examine the general inductive\n",
    "case. As mentioned above, the inductive case will be rather similar to the\n",
    "special case with $\\tau=2$.\n",
    "\n",
    "Let $\\tau$ be an arbitrary decision horizon. Assume that we are able to compute\n",
    "the value of any dataset with a horizon of $\\tau - 1$. Suppose we have an\n",
    "arbitrary dataset $\\mathcal{D}$ and we need to decide where the next observation\n",
    "should be made. In this section, we will review how to do this in an optimal\n",
    "manner and how to compute its value.\n",
    "\n",
    "From Garnett, the $\\tau$-step expected utility gain from observing $x$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(x;\\mathcal{D}) = \\mathbb{E}\\left[\n",
    "        u(\\mathcal{D}_\\tau)\\vert x,\\mathcal{D}\\right] - u(\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "We wish to maximize the expected utility gain given above. Just as in the\n",
    "special case, we can express this using _shorter-horizon quantities_ using a\n",
    "telescoping sum:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(x;\\mathcal{D}) = \\alpha_1(x;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_{\\tau-1}(x_2;\\mathcal{D}_1)\\vert x,\\mathcal{D}\n",
    "    \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Then, if we knew the corresponding value $y$ (and therefore $\\mathcal{D}_1$),\n",
    "the assumption of optimal behavior provides an expected further gain of\n",
    "$\\alpha_{\\tau-1}^*(\\mathcal{D}_1)$ which is a quantity we are able to compute\n",
    "via the inductive hypothesis. Therefore, if we assume optimal behavior for all\n",
    "future decisions, we can express the expected utility gain:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(x;\\mathcal{D})=\\alpha_1(x;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_{\\tau-1}^*(\\mathcal{D}_1)\\vert x,\\mathcal{D}\n",
    "    \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "To determine the optimal decision and the $\\tau$-step value of the dataset,\n",
    "we maximize the expected utility gain at step $\\tau$:\n",
    "\n",
    "\\begin{equation*}\n",
    "    x \\in \\argmax_{x^\\prime\\in\\mathcal{X}}\\alpha_\\tau(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau^*(\\mathcal{D}) = \\max_{x^\\prime\\in\\mathcal{X}}\n",
    "    \\alpha_\\tau(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "This concludes our analysis and shows that we can attain optimal behavior for a\n",
    "horizon of $\\tau$ given a dataset $\\mathcal{D}$ and compute its value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873645a7",
   "metadata": {},
   "source": [
    "#### Bellman's Principle of Optimality and the Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b426b05",
   "metadata": {},
   "source": [
    "If we substitute the expected utility gain (expressed using shorter-horizon\n",
    "quantities) into the the final expression above (maximizing the expected utility\n",
    "gain at step $\\tau$), we obtain the **Bellman equation**:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau^*(\\mathcal{D}) = \\max_{x^\\prime\\in\\mathcal{X}}\n",
    "    \\{\\alpha_1(x^\\prime;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_{\\tau-1}^*(\\mathcal{D}_1)\\vert x^\\prime,\\mathcal{D}\n",
    "    \\right]\\}\n",
    "\\end{equation*}\n",
    "\n",
    "The Bellman equation forms the recursive definition of the value in terms of\n",
    "the value of future data. It is a key result in the theory of optimal sequential\n",
    "decisions. In particular, it reflects **Bellman's Principle of Optimality**\n",
    "which states that we always act optimally to maximize the expected terminal\n",
    "utility given the available data. More generally, it characterizes optimal\n",
    "policies in terms of the optimality of sub-policies.\n",
    "\n",
    "Here is a quote provided by Garnett from Bellman's _Dynamic Programming_ book:\n",
    "\n",
    "_An optimal policy has the property that whatever the initial state and initial_\n",
    "_decision are, the remaining decisions must constitute an optimal policy with_\n",
    "_regard to the state resulting from the first decision_.\n",
    "\n",
    "Therefore, in order to create a sequence of optimal decisions, we choose the\n",
    "first decision optimally and then choose all future decisions optimally given\n",
    "the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d4f8e",
   "metadata": {},
   "source": [
    "## Cost of the Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2097e2",
   "metadata": {},
   "source": [
    "While the framework introduced in the previous sections is theoretically simple,\n",
    "actually computing the optimal policy is prohibitive. The only exception is if\n",
    "we are computing the policy for very short decision horizons.\n",
    "\n",
    "To show this barrier, recall the expression for the expected utility gain where\n",
    "we assume optimal behavior (with $\\tau=2$):\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_2(x;\\mathcal{D})=\\alpha_1(x;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_1^*(\\mathcal{D}_1)\\vert x,\\mathcal{D}\n",
    "    \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "As Garnett notes, even though the second term appears to be a rather standard\n",
    "expectation over the random variable $y$, evaluating this requires solving a\n",
    "_non-trivial_ global optimization problem:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_1^*(\\mathcal{D}) =\n",
    "    \\max_{x^\\prime\\in\\mathcal{X}}\\alpha_1(x^\\prime;\\mathcal{D})\n",
    "\\end{equation*}\n",
    "\n",
    "Furthermore, even if we are only considering a horizon with two decisions left,\n",
    "we would need to solve a dually nested global optimization problem (no simple\n",
    "feat!). Similarly, from the recursively defined optimal policy, we can see that\n",
    "if the horizon is $\\tau$ then we need to solve $\\tau$ nested optimization\n",
    "problems to attain the optimal decision. Here we show this, following the\n",
    "temporary compact notation used by Garnett:\n",
    "\n",
    "\\begin{equation*}\n",
    "    x \\in \\argmax \\alpha_\\tau\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        \\alpha_\\tau &= \\alpha_1 + \\mathbb{E}\\left[\\alpha_{\\tau-1}^*\\right]\n",
    "        \\\\\n",
    "        &= \\alpha_1 + \\mathbb{E}\\left[\\max \\alpha_{\\tau-1}\\right]\n",
    "        \\\\\n",
    "        &= \\alpha_1 + \\mathbb{E}\\left[\\max\\{\\alpha_1 + \n",
    "            \\mathbb{E}\\left[\\alpha_{\\tau-2}^*\\right]\\}\\right]\n",
    "        \\\\\n",
    "        &= \\alpha_1 + \\mathbb{E}\\left[\\max\\{\\alpha_1 +\n",
    "            \\mathbb{E}\\left[\\max\\{\\alpha_1\n",
    "            \\mathbb{E}\\left[\\max\\{\\alpha_1 + \\dots\\right]\\}\\right]\\}\\right]\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "This means that the design of any optimal decision would require repeated\n",
    "maximization over the domain plus expectation over unknown observations until\n",
    "we reach the horizon. See Figure 5.4 in Garnett's text for a visualization of\n",
    "this problem as a decision tree. There we can clearly see that each unknown\n",
    "quantity will contribute a significant branching factor and that computing the\n",
    "expected utility at $x$ will require a traversal of the entire tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7e2ff",
   "metadata": {},
   "source": [
    "We can use this structure to determine the cost of computing the optimal policy,\n",
    "which obviously grows with the horizon. Here, we outline the running time\n",
    "analysis for a naïve implementation via exhaustive traversal of the decision\n",
    "tree provided by Garnett.\n",
    "\n",
    "Suppose that for each maximization we use an optimization routine and for each\n",
    "expectation we use a numerical quadrature routine.\n",
    "\n",
    "Allowing $n$ evaluations of the objective each time the optimizer is called and\n",
    "$q$ observations of the integrand per call to the quadrature routine, each\n",
    "decision along the horizon will contribute a multiplicative factor of\n",
    "$\\mathcal{O}(nq)$ to the total running time. Therefore, the amount of work\n",
    "required to compute the optimal decision with a horizon of $\\tau$ is\n",
    "$\\mathcal{O}(n^{\\tau}q^{\\tau})$. Clearly, the running time will grow\n",
    "exponentially with respect to the horizon.\n",
    "\n",
    "In the next section, we discuss how to avoid this prohibitive barrier by\n",
    "approximating the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995af24",
   "metadata": {},
   "source": [
    "## Approximation of the Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83d56d",
   "metadata": {},
   "source": [
    "Due to the exponential growth of the running time with respect to the horizon,\n",
    "the computational work required to obtain the optimal policy becomes\n",
    "intractable. However, we can utilize general approximation schemes to compute\n",
    "the optimal policy. These schemes are methods deeply studied in **approximate**\n",
    "**dynamic programming**.\n",
    "\n",
    "Recall the _intractable_ optimal expected marginal gain which is given by\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(x;\\mathcal{D}) = \\alpha_1(x;\\mathcal{D}) + \\mathbb{E}\\left[\n",
    "        \\alpha_{\\tau-1}^*(\\mathcal{D}_1)\\vert x,\\mathcal{D}\n",
    "    \\right].\n",
    "\\end{equation*}\n",
    "\n",
    "To avoid the difficult part of the expression, the recursively defined future\n",
    "value $\\alpha^*$, we can substitute in a tractable approximation. While this\n",
    "will induce a suboptimal and approximate policy, it is still rationally guided.\n",
    "There are two specific approximation schemes that are commonly used in Bayesian\n",
    "optimization: _limited lookahead_ and _rollout_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10398037",
   "metadata": {},
   "source": [
    "### Limited Lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dabc306",
   "metadata": {},
   "source": [
    "The idea behind the **limited lookahead** approximation scheme is to, as the\n",
    "name suggests, limit how far into the future we look when making decisions.\n",
    "Specifically, we restrict how many future observations we will consider in each\n",
    "decision. Such an approach is very practical since the closer decisions are to\n",
    "termination, the (significantly) less computation required compared to earlier\n",
    "decisions.\n",
    "\n",
    "With this reasoning, we will develop a family of approximations to the optimal\n",
    "policy defined by \"artificially\" limiting the horizon used during optimization\n",
    "to a feasible maximum $\\ell$. Thus, if we face an infeasible decision horizon\n",
    "$\\tau$, then we use the approximation\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(x;\\mathcal{D}) \\approx \\alpha_\\ell(x;\\mathcal{D}).\n",
    "\\end{equation*}\n",
    "\n",
    "When we maximize this score, we will act optimally under the _incorrect but_\n",
    "_convenient assumption_ that there are only $\\ell$ observations left.\n",
    "Effectively, this assumes $u(\\mathcal{D}_\\tau)\\approx u(\\mathcal{D}_\\ell)$. This\n",
    "scheme is often regarded (sometimes in a disparaging manner) as _myopic_ due to\n",
    "the fact that we limit ourselves to only considering the next few observations\n",
    "on the horizon instead of viewing the entire horizon.\n",
    "\n",
    "An $\\ell$**-step lookahead policy** is a policy that selects each observation to\n",
    "maximize the limited-horizon acquisition function, denoted\n",
    "$\\alpha_{\\min\\{\\ell,\\tau\\}}$. It is also considered a _rolling horizon strategy_\n",
    "since the truncated horizon \"rolls along\" with us as we continue.\n",
    "\n",
    "Considering computational complexity, we are able to bound the effort required\n",
    "when we limit the horizon. The effort is bounded to at most\n",
    "$\\mathcal{O}(n^\\ell q^\\ell)$ for each decision. This can be a major speedup,\n",
    "particularly when the observation (search) budget is significantly larger than\n",
    "the selected maximum lookahead, $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4028d2",
   "metadata": {},
   "source": [
    "#### One-Step Lookahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a971e5b0",
   "metadata": {},
   "source": [
    "A special case of the limited lookahead approach is the **one-step lookahead**\n",
    "method which is very important in Bayesian optimization. Since it aims to\n",
    "successively maximize the expected marginal gain from acquiring one more\n",
    "observation ($\\alpha_1$), it is often possible to derive closed-form,\n",
    "analytically differentiable expressions for $\\alpha_1$. This makes it the most\n",
    "efficient lookahead approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8efcb",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3855d27b",
   "metadata": {},
   "source": [
    "In our theoretical exploration, the optimal policy will evaluate a candidate\n",
    "observation point by simulating the rest of the optimization after that\n",
    "decision, under the recursive assumption that we decide optimally for every\n",
    "future decision. While rational, it's intractable. The **rollout** approach\n",
    "emulates the structure of the optimal policy but instead uses a tractable\n",
    "_suboptimal_ policy to simulate future decisions.\n",
    "\n",
    "Specifically, given another observation $(x,y)$, we use an inexpensive _base_ or\n",
    "_heuristic_ policy to simulate a reasonable but potentially suboptimal\n",
    "realization of the next decision $x_2$. Then, we take an expectation with\n",
    "respect to the unknown value at $x_2$, $y_2$. We continue forward, using the\n",
    "base policy to select another point, $x_3$, and so forth until we reach the\n",
    "decision horizon. Since this approach does not lead to branching in the tree\n",
    "for this decision, we avoid the expensive subtree that would be required by the\n",
    "optimal policy. Instead, we use the terminal utilities from the resulting\n",
    "pruned tree to estimate the expected marginal gain $\\alpha_\\tau$ that we\n",
    "maximize as a function of $x$.\n",
    "\n",
    "While there are not any restrictions on how we design the base policy, since the\n",
    "point of this approximation is to improve efficiency, the base policy design\n",
    "should be something fairly efficient. One common and typically effective choice\n",
    "pointed out by Garnett is to simulate the future decisions using the one-step\n",
    "lookahead approach. That way, if we use off-the-shelf optimizers and quadrature\n",
    "routines to traverse the resulting, rollout decision tree (using one-step\n",
    "lookahead as the base policy), the computational complexity of the policy with\n",
    "decision horizon $\\tau$ is $\\mathcal{O}(n^2 q^\\tau)$. As Garnett points out,\n",
    "this is considerably faster than the optimal policy and while we still have\n",
    "exponential growth with respect to the number of observations ($q$), in most\n",
    "cases $q << n$.\n",
    "\n",
    "Furthermore, the flexibility in the design of the base policy makes the rollout\n",
    "approach an extremely flexible policy approximation scheme. One example provided\n",
    "by Garnett is if we were to combine rollout with limited lookahead to attain\n",
    "approximate policies with _tunable running time_. In particular, we can view\n",
    "$\\ell$-step lookahead as a special case of rollout where the base policy \n",
    "designs the $\\ell-1$ future decisions optimally assuming a myopic horizon and\n",
    "then _terminates early_, ignoring any observations left in the budget.\n",
    "\n",
    "Additionally, we could use a base policy that designs all of the remaining\n",
    "observations in the budget _simultaneously_. By ignoring the dependence between\n",
    "these decisions, we can achieve a computational advantage while still being\n",
    "aware of the horizon. These **batch rollout** schemes have been shown to work\n",
    "well in Bayesian optimization and while we account for the entire horizon, the\n",
    "resulting tree depth is still much less compared to the optimal policy tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40208d90",
   "metadata": {},
   "source": [
    "## Cost-Aware Optimization and Treating Termination as a Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fcc81b",
   "metadata": {},
   "source": [
    "Up to this point, we have only considered optimization policies that are under\n",
    "a pre-defined and constant budget for the maximum number of observations. While\n",
    "this setup is common, it is not universal and in some cases we may want to\n",
    "leverage our changing beliefs about the objective function to help us decide\n",
    "_dynamically_ when termination is the best decision.\n",
    "\n",
    "The idea of dynamic termination is advantageous when we want to account for the\n",
    "cost of acquiring data during optimization. For example, if the cost of gaining\n",
    "more data varies across the search space then it makes no sense to define our\n",
    "budget based on the number of evaluations. Instead, we can account for the\n",
    "acquisition costs in the utility function to explicitly reason about the\n",
    "cost-benefit tradeoff for each additional observation. If the cost of acquiring\n",
    "an additional point outweighs its expected benefit it may provide, we can seek\n",
    "to terminate the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7dea5a",
   "metadata": {},
   "source": [
    "### Modeling Termination as a Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df5a5e",
   "metadata": {},
   "source": [
    "To model dynamic termination, we modify the previously defined sequential\n",
    "decisions case with a pre-defined and constant budget. Now, we will allow\n",
    "ourselves to terminate optimization at any point, as we see fit.\n",
    "\n",
    "Suppose we are performing optimization and have already acquired data\n",
    "$\\mathcal{D}$. Unlike the known-budget case where we would need to decide where\n",
    "to sample next, we face a new decision: is it best to terminate optimization and\n",
    "return dataset $\\mathcal{D}$? If not, where should we sample next?\n",
    "\n",
    "We can model this as a decision problem under uncertainty with the action space\n",
    "being the domain $\\mathcal{X}$ but now we will augment the action space with a\n",
    "special additional action $\\emptyset$ representing termination:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{A} = \\mathcal{X} \\cup \\{\\emptyset\\}\n",
    "\\end{equation*}\n",
    "\n",
    "Note that we will follow Garnett's recommendation to model the decision process\n",
    "as not actually terminating after the termination action is selected but rather\n",
    "continuing with a collapsed action space: $\\mathcal{A} = \\{\\emptyset\\}$ (once we\n",
    "terminate, we cannot go back)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3059b6",
   "metadata": {},
   "source": [
    "We could derive the optimal optimization policy for dynamic termination using\n",
    "induction but we need to address the issue of the base case (representing the\n",
    "\"final\" decision) breaking down once we allow the potential for a\n",
    "non-terminating sequence of decisions. To address this, we will operate under\n",
    "the assumption that there is a fixed, known upper-bound $\\tau_{\\max}$ on the\n",
    "total number of observations we can make. Upon reaching this bound, the\n",
    "optimization will terminate no matter what.\n",
    "\n",
    "Now that we assume the decision process is bounded, the inductive argument\n",
    "derived in the known-budget applies although we need to re-define how we will\n",
    "compute the value of the termination action. Luckily, this is fairly\n",
    "obvious: since termination does not augment our data and no actions can be\n",
    "taken after we terminate, the expected marginal gain from termination will\n",
    "always be zero:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha_\\tau(\\emptyset;\\mathcal{D}) = 0\n",
    "\\end{equation*}\n",
    "\n",
    "Now, apart from substituting $\\mathcal{A} for \\mathcal{X}$ in the previous set\n",
    "of derived expressions, we obtain the optimal policy for the case of dynamic\n",
    "termination.\n",
    "\n",
    "See Figure 5.8 in Garnett's text for an illustration of one-step lookahead with\n",
    "the option to terminate. In this example, the optimization policy accounts for\n",
    "the cost of observations across the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98cf8ac",
   "metadata": {},
   "source": [
    "### Considering Location-Dependent Observation Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaa38c",
   "metadata": {},
   "source": [
    "Suppose that the cost of acquiring an observation depends on the location and is\n",
    "defined by a known cost function $c(x)$. In practice, the observation cost\n",
    "function could be unknown or stochastic (this will be examined in other\n",
    "notebooks and in Garnett's Chapter 11).\n",
    "\n",
    "To explicitly reason about observation costs, location-dependent or otherwise,\n",
    "we can change our approach to the utility function. One rather natural approach\n",
    "would be to select a utility function that exclusively measures the returned\n",
    "dataset's quality (i.e., ignores any costs incurred during acquisition). This is\n",
    "referred to as the **data utility**, denoted by $u^\\prime(\\mathcal{D})$, and is\n",
    "parallel to the cost-agnostic utility from the known-budget case.\n",
    "\n",
    "Next, we adjust the data utility to consider the acquisition cost of\n",
    "observations. In several applications, the acquisitions costs are additive\n",
    "meaning the cost of acquiring an arbitrary dataset $\\mathcal{D}$ is:\n",
    "\n",
    "\\begin{equation*}\n",
    "    c(\\mathcal{D}) = \\sum_{x\\in\\mathcal{D}}c(x)\n",
    "\\end{equation*}\n",
    "\n",
    "If we express the cost of acquisition and data utility in the same units (i.e.,\n",
    "monetary units such as dollars), then we could evaluate a dataset based on its\n",
    "utility and cost via the **cost-adjusted utility**:\n",
    "\n",
    "\\begin{equation*}\n",
    "    u(\\mathcal{D}) = u^\\prime(\\mathcal{D}) - c(\\mathcal{D})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f2510",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c3d27",
   "metadata": {},
   "source": [
    "For full-reference details, the BibTeX entries can be found in the\n",
    "`bibliography.bib` file.\n",
    "\n",
    " * _Bayesian Optimization_ by Roman Garnett (2023)\n",
    " * _Bayesian Optimization: Theory and Practice Using Python_ by Peng Liu (2023)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
